{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\siddh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\siddh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 20.953706741333008\n",
      "Epoch 1, Mean Squared Error: 20.953706741333008\n",
      "Mean Squared Error: 15.653674125671387\n",
      "Epoch 2, Mean Squared Error: 15.653674125671387\n",
      "Mean Squared Error: 11.970582008361816\n",
      "Epoch 3, Mean Squared Error: 11.970582008361816\n",
      "Mean Squared Error: 9.387707710266113\n",
      "Epoch 4, Mean Squared Error: 9.387707710266113\n",
      "Mean Squared Error: 7.2328033447265625\n",
      "Epoch 5, Mean Squared Error: 7.2328033447265625\n",
      "Mean Squared Error: 5.597498416900635\n",
      "Epoch 6, Mean Squared Error: 5.597498416900635\n",
      "Mean Squared Error: 4.148698329925537\n",
      "Epoch 7, Mean Squared Error: 4.148698329925537\n",
      "Mean Squared Error: 3.0221729278564453\n",
      "Epoch 8, Mean Squared Error: 3.0221729278564453\n",
      "Mean Squared Error: 2.181945562362671\n",
      "Epoch 9, Mean Squared Error: 2.181945562362671\n",
      "Mean Squared Error: 1.5818978548049927\n",
      "Epoch 10, Mean Squared Error: 1.5818978548049927\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"C:/Users/siddh/Desktop/BERT/New folder/CELA.csv\")\n",
    "\n",
    "# Extract essay texts and labels\n",
    "essays = df['Essays'].tolist()\n",
    "labels = df[['Grammar', 'Lexical', 'Global Organization', 'Local Organization', 'Supporting Ideas', 'Holistic']].values\n",
    "\n",
    "# Tokenization function\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return tokens['input_ids'], tokens['attention_mask']\n",
    "\n",
    "# Prepare data function\n",
    "def prepare_data(texts, labels):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for text in texts:\n",
    "        tokens = tokenize_text(text)\n",
    "        input_ids.append(tokens[0])\n",
    "        attention_masks.append(tokens[1])\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)  # Ensure labels are of the correct type\n",
    "\n",
    "    return input_ids, attention_masks, labels\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(essays, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the BERT model for regression on each target\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Prepare the training data\n",
    "input_ids, attention_masks, labels = prepare_data(essays, labels)\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "train_dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Prepare the validation data\n",
    "val_input_ids, val_attention_masks, val_labels = prepare_data(val_texts, val_labels)\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            all_preds.extend(logits.cpu().numpy())\n",
    "            all_labels.extend(batch[2].cpu().numpy())\n",
    "        mse = mean_squared_error(all_labels, all_preds)\n",
    "        print(f'Mean Squared Error: {mse}')\n",
    "    return mse\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "best_val_loss = float('inf')  # Initialize with a large value for comparison\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate on the validation set\n",
    "    val_loss = evaluate_model(model, val_dataloader)\n",
    "\n",
    "    # Print or log the learning rate if desired\n",
    "    print(f'Epoch {epoch + 1}, Mean Squared Error: {val_loss}')\n",
    "\n",
    "    # Save the model if validation loss improves\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model_save_path = f\"essay_scoring_model_regression_{timestamp}\"\n",
    "        model.save_pretrained(model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.495993  4.8120184 4.601542  4.7985306 4.8196936 3.4043941]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return tokens['input_ids'], tokens['attention_mask']\n",
    "\n",
    "\n",
    "def prepare_data(texts, labels):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for text in texts:\n",
    "        tokens = tokenize_text(text)\n",
    "        input_ids.append(tokens[0])\n",
    "        attention_masks.append(tokens[1])\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)  # Ensure labels are of the correct type\n",
    "\n",
    "    return input_ids, attention_masks, labels\n",
    "\n",
    "# Load the saved model\n",
    "model_path = \"C:\\\\Users\\\\siddh\\\\Desktop\\\\BERT\\\\New folder\\\\essay_scoring_model_regression_20240228_123826\"  # Replace <timestamp> with the actual timestamp\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Tokenize the input essays\n",
    "input_essays = [\"With the development of our city, human right is talked by everyone of us more and more often, and for the city, two kids allowed to be born, is another stone throw into the lake, and that means the number of people will get a new chance to up to a amazing point, by the way, can we ask us a simple question: what are we going to sacrifice again to tell what's right or what's wrong with us. we only see high speed grow up of our city, but we never see the cut down tree, the populated watered, we can not put up a banner on a lonely tree surrounded by stumps, the go up number of people will lead to many problem, which many person think is the outstanding of human right. It's not what we say but what we that matters, we can't bring the whole world into mad then thinking what we could do to save it.\"]\n",
    "\n",
    "# Prepare input tensors\n",
    "input_ids, attention_masks, _ = prepare_data(input_essays, [])\n",
    "\n",
    "# Make predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    inputs = {'input_ids': input_ids, 'attention_mask': attention_masks}\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = logits.cpu().numpy()\n",
    "\n",
    "# Post-process the predictions if necessary\n",
    "# For example, if you want to get the final predicted scores for each aspect:\n",
    "predicted_scores = predictions.squeeze()  # Squeeze removes the extra dimension added by batch processing\n",
    "\n",
    "# Now predicted_scores contains the predicted scores for each aspect\n",
    "print(predicted_scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
